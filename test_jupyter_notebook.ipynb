{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import sum \n",
    "from numpy import matmul\n",
    "from numpy import multiply\n",
    "from numpy import dot\n",
    "from numpy import exp\n",
    "from numpy import log\n",
    "from numpy import abs\n",
    "from numpy import min\n",
    "from numpy import transpose\n",
    "from numpy.core.function_base import linspace\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "import time\n",
    "\n",
    "#Loading in the MNIST data\n",
    "file = loadmat('/Users/samsuidman/Desktop/neurophysics/machine_learning/mnistAll.mat') \n",
    "\n",
    "#Preparing the data\n",
    "train_images = file['mnist']['train_images'][0][0]\n",
    "train_labels = file['mnist']['train_labels'][0][0].transpose()[0]\n",
    "test_images = file['mnist']['test_images'][0][0]\n",
    "test_labels = file['mnist']['test_labels'][0][0].transpose()[0]\n",
    "indices_3 = np.where(train_labels==3)[0]\n",
    "indices_7 = np.where(train_labels==7)[0]\n",
    "indices_3_test = np.where(test_labels==3)[0]\n",
    "indices_7_test = np.where(test_labels==7)[0]\n",
    "X3 = train_images[:,:,indices_3]\n",
    "X7 = train_images[:,:,indices_7]\n",
    "X3_test = test_images[:,:,indices_3_test]\n",
    "X7_test = test_images[:,:,indices_7_test]\n",
    "n3 = np.size(X3,2)\n",
    "n7 = np.size(X7,2)\n",
    "n3_test = np.size(X3_test,2)\n",
    "n7_test = np.size(X7_test,2)\n",
    "X3 = np.reshape(X3,[784,n3])\n",
    "X7 = np.reshape(X7,[784,n7])\n",
    "X3_test = np.reshape(X3_test,[784,n3_test])\n",
    "X7_test = np.reshape(X7_test,[784,n7_test])\n",
    "X3 = X3/np.max((np.max(np.concatenate(X3)),np.max(np.concatenate(X7))))\n",
    "X7 = X7/np.max((np.max(np.concatenate(X3)),np.max(np.concatenate(X7))))\n",
    "X3_test = X3_test/np.max((np.max(np.concatenate(X3_test)),np.max(np.concatenate(X7_test))))\n",
    "X7_test = X7_test/np.max((np.max(np.concatenate(X3_test)),np.max(np.concatenate(X7_test))))\n",
    "X3 = (np.insert(X3,0,1,axis=0)).transpose() #shape = (6131,785)\n",
    "X7 = (np.insert(X7,0,1,axis=0)).transpose() #shape = (6265,785)\n",
    "X3_test = (np.insert(X3_test,0,1,axis=0)).transpose() #shape = (1010,785)\n",
    "X7_test = (np.insert(X7_test,0,1,axis=0)).transpose() #shape = (1028,785)\n",
    "t3 = np.zeros([1,n3])[0] #length = 6131\n",
    "t7 = np.ones([1,n7])[0] #length = 6265\n",
    "t3_test = np.zeros([1,n3_test])[0] #length = 1010\n",
    "t7_test = np.ones([1,n7_test])[0] #length = 1028\n",
    "\n",
    "#These are the important matrices and arrays in the end. X3 consists of P=6131 images of the number 3. X[3] for example is the 4th image. Each image has 28x28=784 pixels. To make sure that an image doesn't only consists of zeros, before each image is a 1 added. \n",
    "X = np.concatenate([X3,X7]) #shape = (12396,785)\n",
    "t = np.concatenate([t3,t7]) #shape = (12396,)\n",
    "X_test = np.concatenate([X3_test,X7_test]) #shape = (2038,785)\n",
    "t_test = np.concatenate([t3_test,t7_test]) #shape = (2038,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#these are the functions that are defined in the exercise \n",
    "\n",
    "def y(w,X): #this function takes a random w and a matrix with N patterns and d dimensions (in our case 785 pixels in total) and returns an array that contains for each pattern the probability that the number is a 3 (or 7)\n",
    "    y = 1/(1+exp(-dot(X,w)))\n",
    "    return y \n",
    "\n",
    "def E(w,X,t): #Return the error for w,X and the actual value 3,7 (so actual 1,0)\n",
    "    y0 = y(w,X)\n",
    "    Ew = -sum(t*log(y0)+(1-t)*log(1-y0))/len(X)\n",
    "    return Ew\n",
    "\n",
    "def dE(w,X,t): #gradient for w,X,t\n",
    "    y0 = y(w,X)\n",
    "    dE = matmul((y0-t),X)/len(X)\n",
    "    return dE\n",
    "\n",
    "def H(w,X): #Hessian for w,X\n",
    "    y0 = y(w,X) \n",
    "    X_y = transpose(multiply(transpose(X),y0*(1-y0))) #multiply each pattern in X by the y value of that pattern. This results in a matrix. \n",
    "    H = matmul(transpose(X_y),X)/len(X) #multiply this X*y times X, and sum over all patterns. The new shape is then (785,785)\n",
    "    return H\n",
    "\n",
    "def dE_weight_decay(w,X,t,k): #Gradient for the weight decay excercises\n",
    "    y0 = y(w,X)\n",
    "    dE = matmul((y0-t),X)/len(X)+k/len(w)*w\n",
    "    return dE\n",
    "\n",
    "def H_weight_decay(w,X,k): #Hessian for the weight decay exercises\n",
    "    y0 = y(w,X) \n",
    "    X_y = transpose(multiply(transpose(X),y0*(1-y0))) #multiply each pattern in X by the y value of that pattern. This results in a matrix. \n",
    "    H = matmul(transpose(X_y),X)/len(X) + k/len(w)*np.eye(len(w)) #multiply this X*y times X, and sum over all patterns. The new shape is then (785,785)\n",
    "    return H\n",
    "\n",
    "X_line_search = X.copy()\n",
    "t_line_search = t.copy()\n",
    "def E_line_search(w): #Return the error for w,X and the actual value 3,7 (so actual 1,0)\n",
    "    y0 = 1/(1+np.exp(-np.dot(X_line_search,w)))\n",
    "    Ew = -sum(t_line_search*log(y0)+(1-t_line_search)*log(1-y0))/len(X_line_search)\n",
    "    return Ew\n",
    "\n",
    "def dE_line_search(w): #gradient for w,X,t\n",
    "    y0 = 1/(1+np.exp(-np.dot(X_line_search,w)))\n",
    "    dE = matmul((y0-t_line_search),X_line_search)/len(X_line_search)\n",
    "    return dE\n",
    "\n",
    "\n",
    "\n",
    "#These are other helpful functions\n",
    "\n",
    "def wrong_patterns(w,X,t): #Takes a model w, a dataset X and labels t and return the fraction of misclassified patterns\n",
    "    y0 = y(w,X)\n",
    "    classifications = np.where(y0>0.5,1,0)\n",
    "    wrong_predictions = np.sum(classifications!=t)\n",
    "    wrong_fraction = wrong_predictions/len(X)\n",
    "    return wrong_fraction\n",
    "\n",
    "\n",
    "def visualize(w,X,i): #Takes a model w, a dataset X and a image number and visualizes the image with what the model thinks it represents\n",
    "    digit = int(np.where(y(w,X[i])>0.5,7,3))\n",
    "    plt.imshow(X[i][1:].reshape(28,28))\n",
    "    return digit\n",
    "\n",
    "def show_results(results,t):\n",
    "    print('training set:  ' + 'E=' + str(round(results[0][0],3)) + '   wrong patterns: ' + str(round(100*results[0][1],1)) + '%')\n",
    "    print('test set:      ' + 'E=' + str(round(results[1][0],3)) + '   wrong patterns: ' + str(round(100*results[1][1],1)) + '%')\n",
    "    print('time passed during learning: ' + str(round(t,1)) + 's')\n",
    "\n",
    "\n",
    "#These are the learning algorithms for the exercise \n",
    "\n",
    "def gradient_descent(w1,X,t,e=0.05,runs=10000): #e=learning_rate\n",
    "    T1 = time.time()\n",
    "    w = w1.copy()\n",
    "    for i in range(runs):\n",
    "        w += -e*dE(w,X,t)\n",
    "        if i%100==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "def momentum(w0,X,t,e=0.05,a=0.03,runs=10000): #e=learning_rate, a=momentum_strength, \n",
    "    T1 = time.time()\n",
    "    w = w0.copy()\n",
    "    dw_old = 0\n",
    "    for i in range(runs):\n",
    "        dw_new = -e*dE(w,X,t) + a*dw_old\n",
    "        w += dw_new + dw_old\n",
    "        dw_old = dw_new.copy()\n",
    "        if i%100==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "def weight_decay(w0,X,t,e=0.05,a=0.03,k=0.01,runs=10000): #e=learning_rate, a=momentum_strength, k=weight_decay_factor\n",
    "    T1 = time.time()\n",
    "    w = w0.copy()\n",
    "    dw_old = 0\n",
    "    for i in range(runs):\n",
    "        dw_new = -e*dE(w,X,t) + a*dw_old\n",
    "        w += dw_new + dw_old\n",
    "        dw_old = dw_new.copy()\n",
    "        if i%100==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "def newton_method(w0,X,t,e,a,k,runs): #e=learning_rate, a=momentum_strength, k=weight_decay_factor\n",
    "    T1 = time.time()\n",
    "    w = w0.copy()\n",
    "    for i in range(runs):\n",
    "        w += -np.matmul(inv(H_weight_decay(w,X,t,k)),dE_weight_decay(w,X,t,k))\n",
    "        if i%1==0: \n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "def line_search(w1,X,t,runs=300):\n",
    "    T1 = time.time()\n",
    "    w = w1.copy()\n",
    "    for i in range(runs):\n",
    "        d = -dE_line_search(w)\n",
    "        g = optimize.line_search(E_line_search,dE_line_search,w,d)[0]\n",
    "        w += g*d\n",
    "        if i%10==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "\n",
    "def conjugate_gradient_descent(w1,X,t,runs=200):\n",
    "    T1 = time.time()\n",
    "    w_old = w1.copy() #initialize both on w_old and w(=w_new) on the w1 that you start with \n",
    "    w = w1.copy()\n",
    "    d = 0 #initialize d as 0 such that d is the gradient descent the first time it is used \n",
    "    for i in range(runs):\n",
    "        b = dot(dE(w,X,t)-dE(w_old,X,t),dE(w,X,t))/np.dot(dE(w_old,X,t),dE(w_old,X,t))\n",
    "        d = -dE(w,X,t) + b*d\n",
    "        g = optimize.line_search(E_line_search,dE_line_search,w,d)[0]\n",
    "        w_old = w.copy()\n",
    "        w += g*d\n",
    "        if i%10==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3)))\n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "def stochastic_gradient_descent(w1,X1,t1,e=0.01,div_factor=100,runs=5000): \n",
    "    T1 = time.time()\n",
    "    w = w1.copy()\n",
    "    Xi = np.array(np.array_split(X1,div_factor),dtype=object)\n",
    "    ti = np.array(np.array_split(t1,div_factor),dtype=object)\n",
    "    for i in range(runs):\n",
    "        j = np.random.randint(0,div_factor)\n",
    "        w += -e*dE(w,Xi[j],ti[j]) #waarschijnlijk gaat het in deze stap fout, omdat de som van al die gradient descent termen heel groot is. \n",
    "        if i%100==0:\n",
    "            T2 = time.time()\n",
    "            print(i,str(round(T2-T1,1))+'s','E='+str(round(E(w,X,t),3))) #Ook kijken naar de errorfunctie \n",
    "    T2 = time.time()\n",
    "    dt = T2-T1\n",
    "    return w, dt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#This is executing the script \n",
    "w0 = np.random.normal(0,1,size=X.shape[1]) #initializing a random model w0 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0s E=1.321\n",
      "100 0.2s E=0.671\n",
      "200 0.4s E=0.532\n",
      "300 0.6s E=0.458\n",
      "400 0.8s E=0.402\n"
     ]
    }
   ],
   "source": [
    "w_stochastic_gradient_descent, t_stochastic_gradient_descent = stochastic_gradient_descent(w0,X,t,runs=500) #learning the model via weight decay \n",
    "results_stochastic_gradient_descent = [[E(w_stochastic_gradient_descent,X,t),wrong_patterns(w_stochastic_gradient_descent,X,t)],[E(w_stochastic_gradient_descent,X_test,t_test),wrong_patterns(w_stochastic_gradient_descent,X_test,t_test)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  E=0.456   wrong patterns: 12.5%\n",
      "test set:      E=0.368   wrong patterns: 10.5%\n",
      "time passed during learning: 1.4\n"
     ]
    }
   ],
   "source": [
    "show_results(results_stochastic_gradient_descent,t_stochastic_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  E=0.363   wrong patterns: 9.4%\n",
      "test set:      E=0.416   wrong patterns: 9.7%\n",
      "time passed during learning: 1.0s\n"
     ]
    }
   ],
   "source": [
    "show_results(results_stochastic_gradient_descent,t_stochastic_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1s E=0.745\n",
      "10 1.1s E=0.199\n",
      "20 2.6s E=0.106\n",
      "30 4.0s E=0.07\n",
      "40 4.9s E=0.063\n",
      "50 5.8s E=0.058\n",
      "60 6.8s E=0.052\n",
      "70 7.8s E=0.049\n",
      "80 8.8s E=0.044\n",
      "90 9.9s E=0.043\n",
      "100 11.2s E=0.035\n",
      "110 12.3s E=0.033\n",
      "120 13.5s E=0.031\n",
      "130 15.0s E=0.03\n",
      "140 16.3s E=0.029\n",
      "150 17.3s E=0.027\n",
      "160 18.2s E=0.027\n",
      "170 19.2s E=0.027\n",
      "180 20.1s E=0.026\n",
      "190 21.1s E=0.026\n",
      "200 22.0s E=0.026\n",
      "210 22.9s E=0.026\n",
      "220 23.8s E=0.025\n",
      "230 24.7s E=0.025\n",
      "240 25.6s E=0.025\n",
      "250 26.4s E=0.025\n",
      "260 27.3s E=0.025\n",
      "270 28.1s E=0.025\n",
      "280 29.0s E=0.024\n",
      "290 29.9s E=0.024\n"
     ]
    }
   ],
   "source": [
    "w_line_search, t_line_search = line_search(w0,X,t) #learning the model via weight decay \n",
    "results_line_search = [[E(w_line_search,X,t),wrong_patterns(w_line_search,X,t)],[E(w_line_search,X_test,t_test),wrong_patterns(w_line_search,X_test,t_test)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  E=0.024   wrong patterns: 0.8%\n",
      "test set:      E=0.053   wrong patterns: 1.6%\n",
      "time passed during learning: 30.7s\n"
     ]
    }
   ],
   "source": [
    "show_results(results_line_search,t_line_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aedca058b4155234967fa3da88bf0b29abe2f47d758f3debe6af85bc2e061a5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
